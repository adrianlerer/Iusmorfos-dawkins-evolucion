{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook-header",
   "metadata": {},
   "source": [
    "# External Validation Analysis: Iusmorfos Cross-Country Framework\n",
    "\n",
    "**World-Class Cross-Country Validation**\n",
    "\n",
    "This notebook provides comprehensive external validation of the Iusmorfos framework\n",
    "across different legal systems and cultural contexts, testing generalizability\n",
    "and cross-cultural transferability.\n",
    "\n",
    "## Validation Objectives\n",
    "\n",
    "1. **Cross-System Validation**: Test across Civil Law, Common Law, and Mixed systems\n",
    "2. **Cultural Transferability**: Assess performance across different cultural dimensions\n",
    "3. **Economic Context Adaptation**: Validate across development levels\n",
    "4. **Temporal Generalization**: Test consistency across different time periods\n",
    "5. **Crisis Response Patterns**: Validate crisis-innovation relationships globally\n",
    "\n",
    "## Target Countries\n",
    "\n",
    "- **üá®üá± Chile**: Civil law system, similar cultural context to Argentina\n",
    "- **üáøüá¶ South Africa**: Mixed legal system, different economic context\n",
    "- **üá∏üá™ Sweden**: Civil law system, Nordic governance model\n",
    "- **üáÆüá≥ India**: Common law system, large population, complex federal structure\n",
    "\n",
    "## Reproducibility Configuration\n",
    "\n",
    "- **Validation Protocol**: Standardized cross-country methodology\n",
    "- **Cultural Metrics**: Hofstede dimensions integration\n",
    "- **Statistical Tests**: Transferability and adaptation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-environment",
   "metadata": {
    "tags": ["parameters"]
   },
   "outputs": [],
   "source": [
    "# Environment Setup and Configuration\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project source to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "print(f\"üåç Iusmorfos External Validation - Project root: {project_root}\")\n",
    "print(f\"‚è∞ Analysis timestamp: {pd.Timestamp.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "# Statistical Analysis\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# External Validation Framework\n",
    "from external_validation import ExternalValidationFramework, LegalSystem\n",
    "from config import get_config\n",
    "\n",
    "# Set up configuration\n",
    "config = get_config()\n",
    "print(f\"‚úÖ Configuration loaded - Seed: {config.config['reproducibility']['random_seed']}\")\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Initialize validation framework\n",
    "validator = ExternalValidationFramework()\n",
    "print(\"üî¨ External validation framework initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "country-data-section",
   "metadata": {},
   "source": [
    "## 1. Country Data Generation and Characteristics\n",
    "\n",
    "Generating synthetic legal innovation data for each target country based on their\n",
    "cultural, economic, and legal system characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate-country-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data for all target countries\n",
    "target_countries = ['CL', 'ZA', 'SE', 'IN']\n",
    "country_datasets = {}\n",
    "country_summaries = {}\n",
    "\n",
    "print(\"üèóÔ∏è Generating country-specific datasets...\")\n",
    "\n",
    "for country_code in target_countries:\n",
    "    # Generate synthetic data\n",
    "    country_data = validator.generate_synthetic_country_data(country_code, n_innovations=350)\n",
    "    country_datasets[country_code] = country_data\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    profile = validator.country_profiles[country_code]\n",
    "    \n",
    "    summary = {\n",
    "        'country_name': profile.name,\n",
    "        'legal_system': profile.legal_system.value,\n",
    "        'gdp_per_capita': profile.gdp_per_capita,\n",
    "        'governance_index': profile.governance_index,\n",
    "        'n_innovations': len(country_data),\n",
    "        'year_range': [int(country_data['year'].min()), int(country_data['year'].max())],\n",
    "        'crisis_proportion': float(country_data['in_crisis'].mean()),\n",
    "        'mean_complexity': float(country_data['complexity_score'].mean()),\n",
    "        'mean_adoption': float(country_data['adoption_success'].mean()),\n",
    "        'mean_fitness': float(country_data['fitness_score'].mean()),\n",
    "        'reform_type_distribution': country_data['reform_type'].value_counts().to_dict()\n",
    "    }\n",
    "    \n",
    "    country_summaries[country_code] = summary\n",
    "    \n",
    "    print(f\"  ‚úÖ {country_code} ({profile.name}): {len(country_data)} innovations, \"\n",
    "          f\"fitness Œº={summary['mean_fitness']:.3f}\")\n",
    "\n",
    "print(f\"\\nüìä Generated datasets for {len(country_datasets)} countries\")\n",
    "\n",
    "# Display country characteristics table\n",
    "characteristics_df = pd.DataFrame({\n",
    "    country: {\n",
    "        'Legal System': summary['legal_system'].replace('_', ' ').title(),\n",
    "        'GDP per Capita': f\"${summary['gdp_per_capita']:,}\",\n",
    "        'Governance Index': f\"{summary['governance_index']:.2f}\",\n",
    "        'Innovations': summary['n_innovations'],\n",
    "        'Crisis %': f\"{summary['crisis_proportion']:.1%}\",\n",
    "        'Mean Fitness': f\"{summary['mean_fitness']:.3f}\"\n",
    "    }\n",
    "    for country, summary in country_summaries.items()\n",
    "}).T\n",
    "\n",
    "print(\"\\nüåç Country Characteristics:\")\n",
    "print(characteristics_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-analysis-section",
   "metadata": {},
   "source": [
    "## 2. Cultural and Legal System Analysis\n",
    "\n",
    "Analysis of cultural dimensions and legal system characteristics that may affect\n",
    "model transferability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract cultural dimensions for visualization\n",
    "cultural_data = {}\n",
    "legal_systems = {}\n",
    "\n",
    "# Include Argentina as baseline\n",
    "argentina_cultural = {\n",
    "    'power_distance': 49,\n",
    "    'individualism': 46, \n",
    "    'masculinity': 56,\n",
    "    'uncertainty_avoidance': 86,\n",
    "    'long_term_orientation': 20\n",
    "}\n",
    "\n",
    "cultural_data['AR'] = argentina_cultural\n",
    "legal_systems['AR'] = 'Civil Law'\n",
    "\n",
    "for country_code in target_countries:\n",
    "    profile = validator.country_profiles[country_code]\n",
    "    cultural_data[country_code] = profile.cultural_dimensions\n",
    "    legal_systems[country_code] = profile.legal_system.value.replace('_', ' ').title()\n",
    "\n",
    "# Create cultural dimensions DataFrame\n",
    "cultural_df = pd.DataFrame(cultural_data).T\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Cross-Country Cultural and Legal Analysis', fontsize=16)\n",
    "\n",
    "# 1. Cultural dimensions radar chart\n",
    "ax1 = axes[0, 0]\n",
    "dimensions = list(cultural_df.columns)\n",
    "countries_to_plot = ['AR', 'CL', 'SE', 'IN']  # Select representative countries\n",
    "\n",
    "angles = np.linspace(0, 2 * np.pi, len(dimensions), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for i, country in enumerate(countries_to_plot):\n",
    "    values = cultural_df.loc[country].tolist()\n",
    "    values += values[:1]  # Complete the circle\n",
    "    \n",
    "    ax1.plot(angles, values, 'o-', linewidth=2, label=f\"{country} ({legal_systems[country]})\", color=colors[i])\n",
    "    ax1.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "\n",
    "ax1.set_xticks(angles[:-1])\n",
    "ax1.set_xticklabels([dim.replace('_', '\\n').title() for dim in dimensions], fontsize=9)\n",
    "ax1.set_ylim(0, 100)\n",
    "ax1.set_title('Cultural Dimensions Comparison')\n",
    "ax1.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "ax1.grid(True)\n",
    "\n",
    "# 2. Cultural distance from Argentina\n",
    "ax2 = axes[0, 1]\n",
    "distances_from_argentina = {}\n",
    "for country in target_countries:\n",
    "    distance = np.mean([abs(cultural_data[country][dim] - argentina_cultural[dim]) \n",
    "                       for dim in dimensions])\n",
    "    distances_from_argentina[country] = distance\n",
    "\n",
    "countries = list(distances_from_argentina.keys())\n",
    "distances = list(distances_from_argentina.values())\n",
    "bars = ax2.bar(countries, distances, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "ax2.set_title('Cultural Distance from Argentina')\n",
    "ax2.set_ylabel('Average Cultural Distance')\n",
    "ax2.set_xlabel('Country')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, distance in zip(bars, distances):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{distance:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. Economic development comparison\n",
    "ax3 = axes[0, 2]\n",
    "gdp_data = {'AR': 10937}  # Argentina baseline\n",
    "governance_data = {'AR': 0.65}\n",
    "\n",
    "for country_code in target_countries:\n",
    "    profile = validator.country_profiles[country_code]\n",
    "    gdp_data[country_code] = profile.gdp_per_capita\n",
    "    governance_data[country_code] = profile.governance_index\n",
    "\n",
    "countries = list(gdp_data.keys())\n",
    "gdp_values = list(gdp_data.values())\n",
    "governance_values = list(governance_data.values())\n",
    "\n",
    "scatter = ax3.scatter(gdp_values, governance_values, \n",
    "                     c=['red', 'blue', 'purple', 'green', 'orange'], \n",
    "                     s=100, alpha=0.7)\n",
    "\n",
    "for i, country in enumerate(countries):\n",
    "    ax3.annotate(country, (gdp_values[i], governance_values[i]), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax3.set_xlabel('GDP per Capita (USD)')\n",
    "ax3.set_ylabel('Governance Index')\n",
    "ax3.set_title('Economic Development vs Governance Quality')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Legal system distribution\n",
    "ax4 = axes[1, 0]\n",
    "legal_system_counts = pd.Series(legal_systems).value_counts()\n",
    "wedges, texts, autotexts = ax4.pie(legal_system_counts.values, labels=legal_system_counts.index, \n",
    "                                  autopct='%1.1f%%', startangle=90)\n",
    "ax4.set_title('Legal System Distribution')\n",
    "\n",
    "# 5. Innovation characteristics by country\n",
    "ax5 = axes[1, 1]\n",
    "innovation_data = []\n",
    "for country_code in ['AR'] + target_countries:\n",
    "    if country_code == 'AR':\n",
    "        # Simulate Argentina data\n",
    "        fitness_mean = 0.65\n",
    "        complexity_mean = 5.2\n",
    "    else:\n",
    "        summary = country_summaries[country_code]\n",
    "        fitness_mean = summary['mean_fitness']\n",
    "        complexity_mean = summary['mean_complexity']\n",
    "    \n",
    "    innovation_data.append({\n",
    "        'Country': country_code,\n",
    "        'Fitness': fitness_mean,\n",
    "        'Complexity': complexity_mean\n",
    "    })\n",
    "\n",
    "innovation_df = pd.DataFrame(innovation_data)\n",
    "scatter = ax5.scatter(innovation_df['Complexity'], innovation_df['Fitness'], \n",
    "                     c=['red', 'blue', 'purple', 'green', 'orange'], s=100, alpha=0.7)\n",
    "\n",
    "for _, row in innovation_df.iterrows():\n",
    "    ax5.annotate(row['Country'], (row['Complexity'], row['Fitness']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax5.set_xlabel('Mean Complexity Score')\n",
    "ax5.set_ylabel('Mean Fitness Score')\n",
    "ax5.set_title('Innovation Characteristics by Country')\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Crisis proportion comparison\n",
    "ax6 = axes[1, 2]\n",
    "crisis_data = {'AR': 0.25}  # Argentina baseline estimate\n",
    "for country_code in target_countries:\n",
    "    crisis_data[country_code] = country_summaries[country_code]['crisis_proportion']\n",
    "\n",
    "countries = list(crisis_data.keys())\n",
    "crisis_proportions = list(crisis_data.values())\n",
    "bars = ax6.bar(countries, crisis_proportions, \n",
    "              color=['red', 'blue', 'purple', 'green', 'orange'], alpha=0.7)\n",
    "ax6.set_title('Crisis Period Proportion by Country')\n",
    "ax6.set_ylabel('Proportion of Innovations During Crises')\n",
    "ax6.set_xlabel('Country')\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, proportion in zip(bars, crisis_proportions):\n",
    "    height = bar.get_height()\n",
    "    ax6.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "             f'{proportion:.1%}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display cultural analysis summary\n",
    "print(\"\\nüé≠ Cultural Analysis Summary:\")\n",
    "print(f\"Most similar to Argentina (culturally): {min(distances_from_argentina, key=distances_from_argentina.get)}\")\n",
    "print(f\"Most different from Argentina: {max(distances_from_argentina, key=distances_from_argentina.get)}\")\n",
    "print(f\"Legal system diversity: {len(set(legal_systems.values()))} different systems\")\n",
    "\n",
    "print(\"\\nüìä Expected Transferability Ranking (cultural similarity):\")\n",
    "sorted_countries = sorted(distances_from_argentina.items(), key=lambda x: x[1])\n",
    "for i, (country, distance) in enumerate(sorted_countries, 1):\n",
    "    profile = validator.country_profiles[country]\n",
    "    print(f\"{i}. {country} ({profile.name}) - Distance: {distance:.1f} - {legal_systems[country]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "validation-execution-section",
   "metadata": {},
   "source": [
    "## 3. Model Validation Execution\n",
    "\n",
    "Running the comprehensive external validation across all target countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validation-execution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive external validation\n",
    "print(\"üöÄ Starting comprehensive external validation...\")\n",
    "\n",
    "validation_results = validator.run_comprehensive_external_validation()\n",
    "\n",
    "print(\"\\n‚úÖ Validation complete! Processing results...\")\n",
    "\n",
    "# Extract key metrics for analysis\n",
    "validation_metrics = {}\n",
    "successful_validations = {}\n",
    "\n",
    "for country_code, result in validation_results['country_results'].items():\n",
    "    if 'error' not in result:\n",
    "        successful_validations[country_code] = result\n",
    "        \n",
    "        metrics = result['performance_metrics']\n",
    "        transferability = result['transferability_metrics']\n",
    "        cultural_adaptation = result['cultural_adaptation']\n",
    "        \n",
    "        validation_metrics[country_code] = {\n",
    "            'country_name': validator.country_profiles[country_code].name,\n",
    "            'r2_score': metrics['r2_score'],\n",
    "            'rmse': metrics['rmse'],\n",
    "            'mae': metrics['mae'],\n",
    "            'transferability_score': transferability['overall_transferability_score'],\n",
    "            'cultural_adaptation_score': cultural_adaptation['cultural_adaptation_score'],\n",
    "            'legal_compatibility': cultural_adaptation['legal_system_compatibility'],\n",
    "            'cultural_distance': cultural_adaptation['overall_cultural_distance'],\n",
    "            'governance_similarity': cultural_adaptation['governance_similarity']\n",
    "        }\n",
    "    else:\n",
    "        print(f\"‚ùå Validation failed for {country_code}: {result.get('error', 'Unknown error')}\")\n",
    "\n",
    "print(f\"\\nüìä Successfully validated on {len(successful_validations)} countries\")\n",
    "\n",
    "# Create validation results DataFrame\n",
    "if validation_metrics:\n",
    "    validation_df = pd.DataFrame(validation_metrics).T\n",
    "    \n",
    "    print(\"\\nüéØ Validation Performance Summary:\")\n",
    "    summary_table = validation_df[['country_name', 'r2_score', 'transferability_score', 'cultural_adaptation_score']].copy()\n",
    "    summary_table['r2_score'] = summary_table['r2_score'].apply(lambda x: f\"{x:.3f}\")\n",
    "    summary_table['transferability_score'] = summary_table['transferability_score'].apply(lambda x: f\"{x:.3f}\")\n",
    "    summary_table['cultural_adaptation_score'] = summary_table['cultural_adaptation_score'].apply(lambda x: f\"{x:.3f}\")\n",
    "    summary_table.columns = ['Country', 'R¬≤ Score', 'Transferability', 'Cultural Adaptation']\n",
    "    \n",
    "    print(summary_table.to_string(index=False))\n",
    "else:\n",
    "    print(\"‚ùå No successful validations to analyze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-analysis-section",
   "metadata": {},
   "source": [
    "## 4. Performance Analysis and Visualization\n",
    "\n",
    "Comprehensive analysis of validation performance across countries and contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not validation_metrics:\n",
    "    print(\"‚ö†Ô∏è No validation metrics available for analysis\")\n",
    "else:\n",
    "    # Create comprehensive performance visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('External Validation Performance Analysis', fontsize=16)\n",
    "    \n",
    "    # 1. R¬≤ Score by Country\n",
    "    ax1 = axes[0, 0]\n",
    "    countries = list(validation_df.index)\n",
    "    r2_scores = validation_df['r2_score'].values\n",
    "    colors = ['blue', 'purple', 'green', 'orange'][:len(countries)]\n",
    "    \n",
    "    bars = ax1.bar(countries, r2_scores, color=colors, alpha=0.7)\n",
    "    ax1.axhline(y=0.6, color='red', linestyle='--', alpha=0.7, label='Good Performance Threshold')\n",
    "    ax1.set_title('Model Performance (R¬≤ Score) by Country')\n",
    "    ax1.set_ylabel('R¬≤ Score')\n",
    "    ax1.set_xlabel('Country')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, r2_scores):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Performance vs Cultural Distance\n",
    "    ax2 = axes[0, 1]\n",
    "    cultural_distances = validation_df['cultural_distance'].values\n",
    "    \n",
    "    scatter = ax2.scatter(cultural_distances, r2_scores, c=colors, s=100, alpha=0.7)\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(cultural_distances) > 2:\n",
    "        z = np.polyfit(cultural_distances, r2_scores, 1)\n",
    "        p = np.poly1d(z)\n",
    "        ax2.plot(cultural_distances, p(cultural_distances), \"r--\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        # Calculate correlation\n",
    "        correlation, p_value = stats.pearsonr(cultural_distances, r2_scores)\n",
    "        ax2.text(0.05, 0.95, f'r = {correlation:.3f}\\np = {p_value:.3f}', \n",
    "                transform=ax2.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        ax2.annotate(country, (cultural_distances[i], r2_scores[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    ax2.set_xlabel('Cultural Distance from Argentina')\n",
    "    ax2.set_ylabel('R¬≤ Score')\n",
    "    ax2.set_title('Performance vs Cultural Distance')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Transferability Score Analysis\n",
    "    ax3 = axes[0, 2]\n",
    "    transferability_scores = validation_df['transferability_score'].values\n",
    "    \n",
    "    bars = ax3.bar(countries, transferability_scores, color=colors, alpha=0.7)\n",
    "    ax3.axhline(y=0.7, color='red', linestyle='--', alpha=0.7, label='High Transferability Threshold')\n",
    "    ax3.set_title('Transferability Score by Country')\n",
    "    ax3.set_ylabel('Transferability Score')\n",
    "    ax3.set_xlabel('Country')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, transferability_scores):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                 f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Legal System Compatibility\n",
    "    ax4 = axes[1, 0]\n",
    "    legal_compatibility = validation_df['legal_compatibility'].values\n",
    "    \n",
    "    bars = ax4.bar(countries, legal_compatibility, color=colors, alpha=0.7)\n",
    "    ax4.set_title('Legal System Compatibility')\n",
    "    ax4.set_ylabel('Compatibility Score')\n",
    "    ax4.set_xlabel('Country')\n",
    "    ax4.set_ylim(0, 1.1)\n",
    "    \n",
    "    # Add legal system labels\n",
    "    for i, (country, bar) in enumerate(zip(countries, bars)):\n",
    "        profile = validator.country_profiles[country]\n",
    "        legal_system = profile.legal_system.value.replace('_', ' ').title()\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2., -0.1,\n",
    "                 legal_system, ha='center', va='top', rotation=45, fontsize=9)\n",
    "    \n",
    "    # 5. Multidimensional Performance Radar\n",
    "    ax5 = axes[1, 1]\n",
    "    \n",
    "    # Prepare radar chart data\n",
    "    metrics_radar = ['R¬≤ Score', 'Transferability', 'Cultural Adaptation', \n",
    "                    'Legal Compatibility', 'Governance Similarity']\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics_radar), endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    # Plot each country\n",
    "    for i, country in enumerate(countries):\n",
    "        values = [\n",
    "            validation_df.loc[country, 'r2_score'],\n",
    "            validation_df.loc[country, 'transferability_score'],\n",
    "            validation_df.loc[country, 'cultural_adaptation_score'],\n",
    "            validation_df.loc[country, 'legal_compatibility'],\n",
    "            validation_df.loc[country, 'governance_similarity']\n",
    "        ]\n",
    "        values += values[:1]\n",
    "        \n",
    "        ax5.plot(angles, values, 'o-', linewidth=2, label=country, color=colors[i])\n",
    "        ax5.fill(angles, values, alpha=0.1, color=colors[i])\n",
    "    \n",
    "    ax5.set_xticks(angles[:-1])\n",
    "    ax5.set_xticklabels(metrics_radar, fontsize=9)\n",
    "    ax5.set_ylim(0, 1)\n",
    "    ax5.set_title('Multidimensional Performance Comparison')\n",
    "    ax5.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax5.grid(True)\n",
    "    \n",
    "    # 6. Performance vs Economic Development\n",
    "    ax6 = axes[1, 2]\n",
    "    gdp_values = [validator.country_profiles[country].gdp_per_capita for country in countries]\n",
    "    \n",
    "    scatter = ax6.scatter(gdp_values, r2_scores, c=colors, s=100, alpha=0.7)\n",
    "    \n",
    "    # Add trend line\n",
    "    if len(gdp_values) > 2:\n",
    "        z = np.polyfit(gdp_values, r2_scores, 1)\n",
    "        p = np.poly1d(z)\n",
    "        x_trend = np.linspace(min(gdp_values), max(gdp_values), 100)\n",
    "        ax6.plot(x_trend, p(x_trend), \"r--\", alpha=0.8, linewidth=2)\n",
    "        \n",
    "        correlation, p_value = stats.pearsonr(gdp_values, r2_scores)\n",
    "        ax6.text(0.05, 0.95, f'r = {correlation:.3f}\\np = {p_value:.3f}', \n",
    "                transform=ax6.transAxes, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    for i, country in enumerate(countries):\n",
    "        ax6.annotate(country, (gdp_values[i], r2_scores[i]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "    \n",
    "    ax6.set_xlabel('GDP per Capita (USD)')\n",
    "    ax6.set_ylabel('R¬≤ Score')\n",
    "    ax6.set_title('Performance vs Economic Development')\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistical analysis summary\n",
    "    print(\"\\nüìà Statistical Analysis Summary:\")\n",
    "    print(f\"Mean R¬≤ Score: {validation_df['r2_score'].mean():.3f} ¬± {validation_df['r2_score'].std():.3f}\")\n",
    "    print(f\"Mean Transferability: {validation_df['transferability_score'].mean():.3f} ¬± {validation_df['transferability_score'].std():.3f}\")\n",
    "    print(f\"Best performing country: {validation_df['r2_score'].idxmax()} (R¬≤ = {validation_df['r2_score'].max():.3f})\")\n",
    "    print(f\"Most transferable: {validation_df['transferability_score'].idxmax()} (Score = {validation_df['transferability_score'].max():.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generalizability-section",
   "metadata": {},
   "source": [
    "## 5. Generalizability Assessment\n",
    "\n",
    "Comprehensive assessment of the framework's generalizability across different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generalizability-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract generalizability assessment from validation results\n",
    "if 'generalizability_assessment' in validation_results:\n",
    "    generalizability = validation_results['generalizability_assessment']\n",
    "    \n",
    "    print(\"üéØ GENERALIZABILITY ASSESSMENT\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    print(f\"\\nüìä Overall Level: {generalizability['generalizability_level'].upper()}\")\n",
    "    \n",
    "    criteria = generalizability['criteria_assessment']\n",
    "    print(f\"\\n‚úÖ Criteria Assessment:\")\n",
    "    print(f\"  Good Performance (R¬≤ > 0.6): {'‚úÖ' if criteria['good_performance'] else '‚ùå'}\")\n",
    "    print(f\"  Consistent Performance (œÉ < 0.15): {'‚úÖ' if criteria['consistent_performance'] else '‚ùå'}\")\n",
    "    print(f\"  High Transferability (> 0.7): {'‚úÖ' if criteria['high_transferability'] else '‚ùå'}\")\n",
    "    print(f\"  Criteria Met: {criteria['criteria_met']}/{criteria['total_criteria']}\")\n",
    "    \n",
    "    metrics = generalizability['quantitative_metrics']\n",
    "    print(f\"\\nüìà Quantitative Metrics:\")\n",
    "    print(f\"  Mean R¬≤: {metrics['mean_r2']:.3f}\")\n",
    "    print(f\"  Standard Deviation: {metrics['std_r2']:.3f}\")\n",
    "    print(f\"  Mean Transferability: {metrics['mean_transferability']:.3f}\")\n",
    "    print(f\"  Countries Validated: {metrics['n_countries_validated']}\")\n",
    "    \n",
    "    if generalizability['limitations']:\n",
    "        print(f\"\\n‚ö†Ô∏è Limitations Identified:\")\n",
    "        for limitation in generalizability['limitations']:\n",
    "            print(f\"  - {limitation}\")\n",
    "    \n",
    "    if generalizability['recommendations']:\n",
    "        print(f\"\\nüí° Recommendations:\")\n",
    "        for rec in generalizability['recommendations'][:5]:  # Show top 5\n",
    "            print(f\"  {rec}\")\n",
    "    \n",
    "    # Create generalizability visualization\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    fig.suptitle('Generalizability Assessment Dashboard', fontsize=14)\n",
    "    \n",
    "    # 1. Criteria fulfillment\n",
    "    ax1 = axes[0]\n",
    "    criteria_names = ['Good\\nPerformance', 'Consistent\\nPerformance', 'High\\nTransferability']\n",
    "    criteria_values = [criteria['good_performance'], criteria['consistent_performance'], criteria['high_transferability']]\n",
    "    colors = ['green' if val else 'red' for val in criteria_values]\n",
    "    \n",
    "    bars = ax1.bar(criteria_names, [1 if val else 0 for val in criteria_values], color=colors, alpha=0.7)\n",
    "    ax1.set_ylim(0, 1.2)\n",
    "    ax1.set_ylabel('Criteria Met')\n",
    "    ax1.set_title('Generalizability Criteria')\n",
    "    \n",
    "    # Add checkmarks/X marks\n",
    "    for bar, met in zip(bars, criteria_values):\n",
    "        symbol = '‚úì' if met else '‚úó'\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., 0.6, symbol, \n",
    "                ha='center', va='center', fontsize=20, fontweight='bold', color='white')\n",
    "    \n",
    "    # 2. Performance distribution\n",
    "    ax2 = axes[1]\n",
    "    if validation_metrics:\n",
    "        r2_scores = [metrics['r2_score'] for metrics in validation_metrics.values()]\n",
    "        ax2.hist(r2_scores, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax2.axvline(x=0.6, color='red', linestyle='--', linewidth=2, label='Performance Threshold')\n",
    "        ax2.axvline(x=np.mean(r2_scores), color='green', linestyle='-', linewidth=2, label='Mean Performance')\n",
    "        ax2.set_xlabel('R¬≤ Score')\n",
    "        ax2.set_ylabel('Frequency')\n",
    "        ax2.set_title('Performance Distribution')\n",
    "        ax2.legend()\n",
    "    \n",
    "    # 3. Transferability radar\n",
    "    ax3 = axes[2]\n",
    "    if validation_metrics:\n",
    "        # Create summary radar for overall assessment\n",
    "        assessment_metrics = ['Performance', 'Consistency', 'Transferability', 'Legal Compatibility']\n",
    "        \n",
    "        performance_score = metrics['mean_r2']\n",
    "        consistency_score = 1.0 - min(metrics['std_r2'] / 0.15, 1.0)  # Invert and normalize\n",
    "        transferability_score = metrics['mean_transferability']\n",
    "        legal_score = np.mean([vm['legal_compatibility'] for vm in validation_metrics.values()])\n",
    "        \n",
    "        values = [performance_score, consistency_score, transferability_score, legal_score]\n",
    "        \n",
    "        angles = np.linspace(0, 2 * np.pi, len(assessment_metrics), endpoint=False).tolist()\n",
    "        angles += angles[:1]\n",
    "        values += values[:1]\n",
    "        \n",
    "        ax3.plot(angles, values, 'o-', linewidth=2, color='blue')\n",
    "        ax3.fill(angles, values, alpha=0.2, color='blue')\n",
    "        ax3.set_xticks(angles[:-1])\n",
    "        ax3.set_xticklabels(assessment_metrics)\n",
    "        ax3.set_ylim(0, 1)\n",
    "        ax3.set_title('Overall Assessment Radar')\n",
    "        ax3.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Generalizability assessment not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-analysis-section",
   "metadata": {},
   "source": [
    "## 6. Comparative Analysis and Insights\n",
    "\n",
    "Deep comparative analysis across legal systems and cultural contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract comparative analysis from validation results\n",
    "if 'comparative_analysis' in validation_results:\n",
    "    comparative = validation_results['comparative_analysis']\n",
    "    \n",
    "    print(\"üîç COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    if 'performance_ranking' in comparative and comparative['performance_ranking']:\n",
    "        print(f\"\\nüèÜ Performance Ranking:\")\n",
    "        for i, country in enumerate(comparative['performance_ranking'], 1):\n",
    "            country_name = validator.country_profiles[country].name\n",
    "            legal_system = validator.country_profiles[country].legal_system.value\n",
    "            r2_score = comparative['performance_comparison'][country]['r2_score']\n",
    "            transferability = comparative['performance_comparison'][country]['transferability_score']\n",
    "            \n",
    "            print(f\"{i}. {country} ({country_name})\")\n",
    "            print(f\"   Legal System: {legal_system.replace('_', ' ').title()}\")\n",
    "            print(f\"   R¬≤ Score: {r2_score:.3f}\")\n",
    "            print(f\"   Transferability: {transferability:.3f}\")\n",
    "    \n",
    "    if 'legal_system_performance' in comparative:\n",
    "        print(f\"\\n‚öñÔ∏è Performance by Legal System:\")\n",
    "        for legal_sys, avg_performance in comparative['legal_system_performance'].items():\n",
    "            print(f\"  {legal_sys.replace('_', ' ').title()}: {avg_performance:.3f} (avg R¬≤)\")\n",
    "    \n",
    "    if 'performance_range' in comparative:\n",
    "        perf_range = comparative['performance_range']\n",
    "        print(f\"\\nüìä Performance Statistics:\")\n",
    "        print(f\"  Best Performance: {perf_range['max_r2']:.3f}\")\n",
    "        print(f\"  Worst Performance: {perf_range['min_r2']:.3f}\")\n",
    "        print(f\"  Performance Range: {perf_range['max_r2'] - perf_range['min_r2']:.3f}\")\n",
    "\n",
    "# Detailed country-specific insights\n",
    "if validation_metrics:\n",
    "    print(f\"\\nüåç COUNTRY-SPECIFIC INSIGHTS\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    for country_code, metrics in validation_metrics.items():\n",
    "        profile = validator.country_profiles[country_code]\n",
    "        validation_result = successful_validations[country_code]\n",
    "        \n",
    "        print(f\"\\nüá´üá∑ {country_code} - {profile.name}\")\n",
    "        print(f\"-\" * 25)\n",
    "        \n",
    "        # Performance summary\n",
    "        print(f\"üìä Performance: R¬≤ = {metrics['r2_score']:.3f}, RMSE = {metrics['rmse']:.3f}\")\n",
    "        \n",
    "        # Determine performance category\n",
    "        if metrics['r2_score'] >= 0.7:\n",
    "            performance_category = \"Excellent\"\n",
    "        elif metrics['r2_score'] >= 0.6:\n",
    "            performance_category = \"Good\"\n",
    "        elif metrics['r2_score'] >= 0.4:\n",
    "            performance_category = \"Moderate\"\n",
    "        else:\n",
    "            performance_category = \"Poor\"\n",
    "        \n",
    "        print(f\"üéØ Category: {performance_category}\")\n",
    "        \n",
    "        # Key characteristics\n",
    "        print(f\"‚öñÔ∏è Legal System: {profile.legal_system.value.replace('_', ' ').title()}\")\n",
    "        print(f\"üí∞ GDP per Capita: ${profile.gdp_per_capita:,}\")\n",
    "        print(f\"üèõÔ∏è Governance Index: {profile.governance_index:.2f}\")\n",
    "        print(f\"üé≠ Cultural Distance: {metrics['cultural_distance']:.1f}\")\n",
    "        \n",
    "        # Transferability insights\n",
    "        transferability_result = validation_result['transferability_metrics']\n",
    "        cultural_result = validation_result['cultural_adaptation']\n",
    "        \n",
    "        print(f\"üîÑ Transferability Score: {metrics['transferability_score']:.3f}\")\n",
    "        \n",
    "        # Adaptation challenges\n",
    "        if 'adaptation_challenges' in cultural_result and cultural_result['adaptation_challenges']:\n",
    "            print(f\"‚ö†Ô∏è Adaptation Challenges:\")\n",
    "            for challenge in cultural_result['adaptation_challenges'][:2]:  # Show top 2\n",
    "                print(f\"   - {challenge}\")\n",
    "        \n",
    "        # Key insights based on performance\n",
    "        if metrics['r2_score'] > 0.6:\n",
    "            print(f\"‚úÖ Strong model transferability - ready for implementation\")\n",
    "        elif metrics['cultural_distance'] > 40:\n",
    "            print(f\"üé≠ High cultural distance may require adaptation\")\n",
    "        elif profile.legal_system != LegalSystem.CIVIL_LAW:\n",
    "            print(f\"‚öñÔ∏è Different legal system may need specialized adaptation\")\n",
    "        else:\n",
    "            print(f\"üìà Moderate performance - investigate specific factors\")\n",
    "\n",
    "# Summary recommendations\n",
    "print(f\"\\nüí° IMPLEMENTATION RECOMMENDATIONS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if validation_metrics:\n",
    "    best_country = max(validation_metrics.keys(), key=lambda k: validation_metrics[k]['r2_score'])\n",
    "    worst_country = min(validation_metrics.keys(), key=lambda k: validation_metrics[k]['r2_score'])\n",
    "    \n",
    "    print(f\"ü•á Priority Implementation: {best_country} ({validator.country_profiles[best_country].name})\")\n",
    "    print(f\"   - Highest validation performance (R¬≤ = {validation_metrics[best_country]['r2_score']:.3f})\")\n",
    "    print(f\"   - Strong transferability ({validation_metrics[best_country]['transferability_score']:.3f})\")\n",
    "    \n",
    "    print(f\"\\nüîß Requires Adaptation: {worst_country} ({validator.country_profiles[worst_country].name})\")\n",
    "    print(f\"   - Lower validation performance (R¬≤ = {validation_metrics[worst_country]['r2_score']:.3f})\")\n",
    "    print(f\"   - Cultural distance: {validation_metrics[worst_country]['cultural_distance']:.1f}\")\n",
    "    \n",
    "    # General recommendations\n",
    "    mean_performance = np.mean([m['r2_score'] for m in validation_metrics.values()])\n",
    "    \n",
    "    if mean_performance >= 0.6:\n",
    "        print(f\"\\n‚úÖ Framework shows strong cross-country generalizability\")\n",
    "        print(f\"üìà Recommend phased international rollout\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è Framework needs improvement for global deployment\")\n",
    "        print(f\"üî¨ Recommend additional cultural adaptation research\")\n",
    "\n",
    "print(f\"\\nüèÅ External Validation Analysis Complete\")\n",
    "print(f\"‚è∞ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-results-section",
   "metadata": {},
   "source": [
    "## 7. Export Results and Metadata\n",
    "\n",
    "Save comprehensive validation results and metadata for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare comprehensive results for export\n",
    "export_results = {\n",
    "    'analysis_metadata': {\n",
    "        'notebook_version': '2.0.0',\n",
    "        'analysis_timestamp': datetime.now().isoformat(),\n",
    "        'config_seed': config.config['reproducibility']['random_seed'],\n",
    "        'target_countries': target_countries,\n",
    "        'successful_validations': len(successful_validations) if 'successful_validations' in locals() else 0\n",
    "    },\n",
    "    'country_profiles': {\n",
    "        code: {\n",
    "            'name': profile.name,\n",
    "            'legal_system': profile.legal_system.value,\n",
    "            'gdp_per_capita': profile.gdp_per_capita,\n",
    "            'governance_index': profile.governance_index,\n",
    "            'cultural_dimensions': profile.cultural_dimensions\n",
    "        }\n",
    "        for code, profile in validator.country_profiles.items()\n",
    "    },\n",
    "    'validation_results': validation_results if 'validation_results' in locals() else {},\n",
    "    'performance_summary': {\n",
    "        'validation_metrics': validation_metrics if 'validation_metrics' in locals() else {},\n",
    "        'country_summaries': country_summaries if 'country_summaries' in locals() else {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert numpy types to native Python types for JSON serialization\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_types(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(item) for item in obj]\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "export_results = convert_numpy_types(export_results)\n",
    "\n",
    "# Save results\n",
    "results_path = config.get_path('results_dir') / f'external_validation_analysis_{config.timestamp}.json'\n",
    "\n",
    "with open(results_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(export_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"üíæ External validation analysis results saved: {results_path}\")\n",
    "\n",
    "# Generate summary report\n",
    "summary_report = f\"\"\"\n",
    "# External Validation Analysis Summary\n",
    "\n",
    "**Analysis Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Framework**: Iusmorfos Cross-Country Validation\n",
    "**Seed**: {config.config['reproducibility']['random_seed']}\n",
    "\n",
    "## Countries Analyzed\n",
    "\n",
    "{''.join([f'- **{code}** ({validator.country_profiles[code].name}): {validator.country_profiles[code].legal_system.value.replace(\"_\", \" \").title()}\\n' for code in target_countries])}\n",
    "\n",
    "## Key Findings\n",
    "\n",
    "{''.join([\n",
    "    f'- **{code}**: R¬≤ = {validation_metrics[code][\"r2_score\"]:.3f}, Transferability = {validation_metrics[code][\"transferability_score\"]:.3f}\\n'\n",
    "    for code in validation_metrics.keys()\n",
    "]) if 'validation_metrics' in locals() else 'No successful validations completed.'}\n",
    "\n",
    "## Generalizability Assessment\n",
    "\n",
    "{validation_results.get('generalizability_assessment', {}).get('generalizability_level', 'Not assessed').upper() if 'validation_results' in locals() else 'Not available'}\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "{''.join([f'- {rec}\\n' for rec in validation_results.get('generalizability_assessment', {}).get('recommendations', [])[:3]]) if 'validation_results' in locals() else 'No recommendations available.'}\n",
    "\n",
    "---\n",
    "*Generated by Iusmorfos External Validation Framework*\n",
    "\"\"\"\n",
    "\n",
    "# Save summary report\n",
    "summary_path = config.get_path('results_dir') / f'external_validation_summary_{config.timestamp}.md'\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"üìÑ Summary report saved: {summary_path}\")\n",
    "\n",
    "print(f\"\\nüåç External Validation Analysis Complete!\")\n",
    "print(f\"üìÅ Results Location: {config.get_path('results_dir')}\")\n",
    "print(f\"üî¨ Framework Status: {'Validated across multiple countries' if 'validation_metrics' in locals() and validation_metrics else 'Requires further validation'}\")\n",
    "print(\"\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "tags": [
   "external-validation",
   "cross-country-analysis",
   "reproducibility",
   "iusmorfos"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}