#!/usr/bin/env python3
"""
Kahneman Prediction Correction Module
Implementa el protocolo de correcci√≥n regresiva para domesticar predicciones intuitivas

Basado en "Thinking, Fast and Slow" - Sistema 2 correction protocol:
1. Estimar resultado promedio (l√≠nea base/tasa base)
2. Hacer predicci√≥n intuitiva (equivalencia de intensidad)  
3. Evaluar correlaci√≥n entre evidencia y outcome
4. Moverse desde l√≠nea base hacia intuici√≥n proporcionalmente a correlaci√≥n

@author: Iusmorfos Universal Framework + Kahneman Corrections
@version: 1.0 - Bias-Corrected Predictions
"""

import numpy as np
import pandas as pd
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Optional, Union
from enum import Enum
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class BaseRateData:
    """
    Datos de tasa base para correcci√≥n regresiva
    Kahneman: "Estimar el resultado promedio (l√≠nea de base)"
    """
    outcome_type: str
    historical_base_rate: float  # Tasa base emp√≠rica hist√≥rica
    sample_size: int  # N casos para calcular tasa base
    confidence_in_base_rate: float  # Confianza en calidad tasa base
    time_period: str  # Per√≠odo temporal de datos
    geographic_scope: str  # √Åmbito geogr√°fico
    
    def is_reliable(self) -> bool:
        """Verifica si tasa base es confiable (N ‚â• 30, confidence ‚â• 0.7)"""
        return self.sample_size >= 30 and self.confidence_in_base_rate >= 0.7

@dataclass 
class IntuitiveEvidence:
    """
    Evidencia para predicci√≥n intuitiva
    Kahneman: "Equivalencia de intensidad de la evidencia"
    """
    evidence_type: str
    evidence_strength: float  # 0-1, fuerza de la evidencia
    evidence_quality: float   # 0-1, calidad/confiabilidad evidencia
    coherence_score: float    # 0-1, coherencia narrativa (Sistema 1)
    availability_bias: float  # 0-1, sesgo disponibilidad detectado
    representativeness_bias: float  # 0-1, sesgo representatividad detectado
    
    def get_bias_adjusted_strength(self) -> float:
        """Ajusta fuerza por sesgos detectados"""
        bias_penalty = (self.availability_bias + self.representativeness_bias) / 2 * 0.3
        return max(0.0, self.evidence_strength - bias_penalty)

@dataclass
class CorrelationAssessment:
    """
    Evaluaci√≥n de correlaci√≥n evidencia-outcome
    Kahneman: "Evaluar correlaci√≥n entre evidencia y resultado"
    """
    evidence_outcome_correlation: float  # -1 to 1, correlaci√≥n estimada
    correlation_confidence: float  # 0-1, confianza en estimaci√≥n correlaci√≥n
    historical_validation: Optional[float] = None  # Correlaci√≥n hist√≥rica si disponible
    expert_consensus: Optional[float] = None  # Consenso expertos sobre correlaci√≥n
    
    def get_reliable_correlation(self) -> float:
        """
        Retorna correlaci√≥n m√°s confiable disponible
        Prioriza validaci√≥n hist√≥rica > consenso experto > estimaci√≥n
        """
        if self.historical_validation and self.correlation_confidence >= 0.7:
            return self.historical_validation
        elif self.expert_consensus and self.correlation_confidence >= 0.5:
            return self.expert_consensus  
        else:
            return self.evidence_outcome_correlation * self.correlation_confidence

class KahnemanPredictionCorrector:
    """
    Corrector de Predicciones basado en protocolos Kahneman
    
    Implementa Sistema 2 thinking para corregir sesgos Sistema 1:
    - Ilusi√≥n de validez ‚Üí Correcci√≥n por calidad evidencia
    - Predicciones no regresivas ‚Üí Regresi√≥n a la media obligatoria
    - Insensibilidad a predictibilidad ‚Üí Evaluaci√≥n expl√≠cita correlaciones
    - Heur√≠stica representatividad ‚Üí Incorporaci√≥n tasas base
    """
    
    def __init__(self):
        self.correction_applied = True
        self.kahneman_protocol_active = True
        
        # Registro de correcciones aplicadas
        self.correction_log: List[Dict] = []
        
        # Umbrales para correcci√≥n (basados en literatura Kahneman)
        self.min_correlation_for_prediction = 0.30  # Correlaci√≥n m√≠nima para predicci√≥n √∫til
        self.max_intuitive_confidence = 0.80  # M√°xima confianza permitida sin correcci√≥n
        self.regression_strength = 0.60  # Factor regresi√≥n hacia media
        
    def correct_prediction(self, 
                          base_rate: BaseRateData,
                          evidence: IntuitiveEvidence, 
                          correlation: CorrelationAssessment,
                          intuitive_prediction: float) -> Dict:
        """
        Aplicar protocolo completo de correcci√≥n Kahneman
        
        Protocolo:
        1. Verificar calidad tasa base
        2. Evaluar sesgos en evidencia  
        3. Validar correlaci√≥n evidencia-outcome
        4. Aplicar correcci√≥n regresiva
        5. Calcular intervalos de confianza honestos
        """
        
        correction_result = {
            'original_intuitive_prediction': intuitive_prediction,
            'corrected_prediction': None,
            'confidence_interval': None,
            'correction_applied': False,
            'kahneman_warnings': [],
            'bias_adjustments': {},
            'methodology': 'kahneman_regressive_correction'
        }
        
        # PASO 1: Verificar tasa base (Kahneman: l√≠nea base)
        if not base_rate.is_reliable():
            correction_result['kahneman_warnings'].append(
                f"Base rate unreliable: N={base_rate.sample_size}, confidence={base_rate.confidence_in_base_rate}"
            )
            # Si tasa base no confiable, usar prior informativo conservador
            baseline = 0.5  # Prior no informativo
            baseline_confidence = 0.3
        else:
            baseline = base_rate.historical_base_rate
            baseline_confidence = base_rate.confidence_in_base_rate
        
        # PASO 2: Ajustar evidencia por sesgos (Sistema 2 override Sistema 1)
        bias_adjusted_evidence = evidence.get_bias_adjusted_strength()
        
        bias_adjustments = {
            'availability_bias_detected': evidence.availability_bias,
            'representativeness_bias_detected': evidence.representativeness_bias,
            'original_evidence_strength': evidence.evidence_strength,
            'bias_adjusted_strength': bias_adjusted_evidence
        }
        correction_result['bias_adjustments'] = bias_adjustments
        
        # PASO 3: Evaluar correlaci√≥n evidencia-outcome
        reliable_correlation = correlation.get_reliable_correlation()
        
        if abs(reliable_correlation) < self.min_correlation_for_prediction:
            correction_result['kahneman_warnings'].append(
                f"Correlation too low for useful prediction: r={reliable_correlation:.3f}"
            )
            # Si correlaci√≥n muy baja, regresar casi completamente a tasa base
            regression_factor = 0.9
        else:
            # Factor de regresi√≥n inversamente proporcional a correlaci√≥n
            regression_factor = 1.0 - abs(reliable_correlation)
        
        # PASO 4: Aplicar correcci√≥n regresiva (Kahneman core protocol)
        # Moverse desde l√≠nea base hacia intuici√≥n proporcionalmente a correlaci√≥n
        prediction_distance = intuitive_prediction - baseline
        corrected_distance = prediction_distance * (1 - regression_factor)
        corrected_prediction = baseline + corrected_distance
        
        correction_result['corrected_prediction'] = corrected_prediction
        correction_result['correction_applied'] = True
        
        # PASO 5: Calcular intervalos de confianza honestos
        # Incorpora incertidumbre de tasa base, evidencia y correlaci√≥n
        base_rate_uncertainty = 1.0 - baseline_confidence
        evidence_uncertainty = 1.0 - evidence.evidence_quality  
        correlation_uncertainty = 1.0 - correlation.correlation_confidence
        
        total_uncertainty = np.sqrt(
            base_rate_uncertainty**2 + 
            evidence_uncertainty**2 + 
            correlation_uncertainty**2
        ) / np.sqrt(3)  # Average uncertainty
        
        # Intervalo de confianza proporcional a incertidumbre
        margin_of_error = total_uncertainty * 0.5  # ¬±50% de incertidumbre total
        
        confidence_interval = (
            max(0.0, corrected_prediction - margin_of_error),
            min(1.0, corrected_prediction + margin_of_error)
        )
        correction_result['confidence_interval'] = confidence_interval
        
        # PASO 6: Warnings adicionales Kahneman
        if intuitive_prediction != corrected_prediction:
            diff = abs(intuitive_prediction - corrected_prediction)
            correction_result['kahneman_warnings'].append(
                f"Intuitive prediction corrected by {diff:.3f} due to regression to base rate"
            )
        
        if evidence.coherence_score > 0.8 and reliable_correlation < 0.5:
            correction_result['kahneman_warnings'].append(
                "High narrative coherence but low predictive correlation - illusion of validity detected"
            )
        
        # Registrar correcci√≥n
        self.correction_log.append({
            'timestamp': pd.Timestamp.now(),
            'base_rate': baseline,
            'intuitive_pred': intuitive_prediction,
            'corrected_pred': corrected_prediction,
            'correlation': reliable_correlation,
            'regression_factor': regression_factor
        })
        
        return correction_result
    
    def detect_prediction_illusions(self, evidence: IntuitiveEvidence) -> List[str]:
        """
        Detectar ilusiones de validez espec√≠ficas (Kahneman)
        """
        illusions = []
        
        # Ilusi√≥n de validez: alta coherencia ‚â† alta validez
        if evidence.coherence_score > 0.8 and evidence.evidence_quality < 0.6:
            illusions.append("Illusion of validity: High coherence but low evidence quality")
        
        # Sesgo de disponibilidad
        if evidence.availability_bias > 0.6:
            illusions.append("Availability bias: Evidence strength may be inflated by memorable examples")
        
        # Sesgo de representatividad  
        if evidence.representativeness_bias > 0.6:
            illusions.append("Representativeness bias: Ignoring base rates in favor of stereotypical matching")
        
        return illusions
    
    def get_correction_statistics(self) -> Dict:
        """Estad√≠sticas de correcciones aplicadas"""
        if not self.correction_log:
            return {"corrections_applied": 0}
        
        df = pd.DataFrame(self.correction_log)
        
        return {
            "corrections_applied": len(df),
            "average_regression_factor": df['regression_factor'].mean(),
            "average_correction_magnitude": (df['intuitive_pred'] - df['corrected_pred']).abs().mean(),
            "predictions_requiring_major_correction": (
                (df['intuitive_pred'] - df['corrected_pred']).abs() > 0.2
            ).sum()
        }

# Funciones de utilidad para aplicar correcciones Kahneman

def create_argentina_base_rates() -> Dict[str, BaseRateData]:
    """
    Crear tasas base hist√≥ricas para Argentina
    Basado en datos hist√≥ricos reales para correcci√≥n regresiva
    """
    return {
        "economic_stabilization": BaseRateData(
            outcome_type="Economic Stabilization Success",
            historical_base_rate=0.45,  # 45% √©xito hist√≥rico Argentina
            sample_size=12,  # 12 intentos estabilizaci√≥n 1980-2020
            confidence_in_base_rate=0.75,  # Datos bien documentados
            time_period="1980-2020",
            geographic_scope="Argentina"
        ),
        
        "structural_reform": BaseRateData(
            outcome_type="Structural Reform Implementation", 
            historical_base_rate=0.35,  # 35% implementaci√≥n exitosa
            sample_size=8,   # 8 grandes reformas estructurales
            confidence_in_base_rate=0.70,
            time_period="1990-2020", 
            geographic_scope="Argentina"
        ),
        
        "constitutional_reform": BaseRateData(
            outcome_type="Constitutional Reform Success",
            historical_base_rate=0.20,  # 20% √©xito (solo 1994)
            sample_size=5,   # 5 intentos reforma constitucional
            confidence_in_base_rate=0.65,
            time_period="1983-2020",
            geographic_scope="Argentina"  
        )
    }

def apply_kahneman_correction_to_argentina_analysis(
    intuitive_predictions: Dict[str, float]
) -> Dict[str, Dict]:
    """
    Aplicar correcciones Kahneman al an√°lisis Argentina
    """
    
    corrector = KahnemanPredictionCorrector()
    base_rates = create_argentina_base_rates()
    corrected_results = {}
    
    for reform_type, intuitive_pred in intuitive_predictions.items():
        
        if reform_type in base_rates:
            base_rate = base_rates[reform_type]
            
            # Simular evidencia actual (en implementaci√≥n real, ser√≠a data real)
            evidence = IntuitiveEvidence(
                evidence_type=f"Current Argentina {reform_type} indicators",
                evidence_strength=0.7,  # Evidencia moderadamente fuerte
                evidence_quality=0.6,   # Calidad de evidencia moderada
                coherence_score=0.8,    # Alta coherencia narrativa (posible ilusi√≥n)
                availability_bias=0.4,  # Sesgo disponibilidad moderado
                representativeness_bias=0.5  # Sesgo representatividad presente
            )
            
            # Correlaci√≥n evidencia-outcome (deber√≠a ser emp√≠rica)
            correlation = CorrelationAssessment(
                evidence_outcome_correlation=0.45,  # Correlaci√≥n moderada
                correlation_confidence=0.6,
                historical_validation=None,  # No disponible a√∫n
                expert_consensus=None        # No disponible a√∫n
            )
            
            # Aplicar correcci√≥n
            correction_result = corrector.correct_prediction(
                base_rate, evidence, correlation, intuitive_pred
            )
            
            corrected_results[reform_type] = correction_result
    
    return corrected_results

# Ejemplo de uso
if __name__ == "__main__":
    
    # Predicciones intuitivas originales (del an√°lisis anterior)
    intuitive_predictions = {
        "economic_stabilization": 0.70,  # 70% predicci√≥n intuitiva
        "structural_reform": 0.55,       # 55% predicci√≥n intuitiva  
        "constitutional_reform": 0.15    # 15% predicci√≥n intuitiva
    }
    
    # Aplicar correcciones Kahneman
    corrected = apply_kahneman_correction_to_argentina_analysis(intuitive_predictions)
    
    print("üß† KAHNEMAN PREDICTION CORRECTIONS APPLIED:")
    print("="*60)
    
    for reform_type, result in corrected.items():
        print(f"\nüìä {reform_type.upper()}:")
        print(f"  Original (Intuitive): {result['original_intuitive_prediction']:.1%}")
        print(f"  Corrected (Regressive): {result['corrected_prediction']:.1%}")
        print(f"  Confidence Interval: {result['confidence_interval'][0]:.1%} - {result['confidence_interval'][1]:.1%}")
        
        if result['kahneman_warnings']:
            print(f"  ‚ö†Ô∏è Warnings: {'; '.join(result['kahneman_warnings'])}")